from airflow import DAG
from airflow.providers.jdbc.hooks.jdbc import JdbcHook
from airflow.operators.python_operator import PythonOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime
import uuid

def generate_unique_key():
    """Generate a 10-digit unique integer key."""
    # Generate a UUID and convert it to a string
    unique_key = str(uuid.uuid4().int)[:10]  # Extract first 10 digits
    return unique_key

def insert_record():
    """Insert a record into DB2 with a unique key and capture the key."""
    jdbc_hook = JdbcHook(jdbc_conn_id='my_jdbc_conn')

    # Generate a 10-digit unique key
    unique_key = generate_unique_key()
    
    # Insert query with unique key
    insert_query = """
        INSERT INTO job_status_table (job_id, status, message, create_time)
        VALUES (?, 'PENDING', 'Job started', CURRENT_TIMESTAMP)
    """
    # Execute the insert query
    jdbc_hook.run(insert_query, parameters=(unique_key,))
    
    # Push the unique_key to XCom for use in later tasks
    return unique_key

def update_record_status(**context):
    """Update the DB2 table based on the Spark job results using the unique key."""
    jdbc_hook = JdbcHook(jdbc_conn_id='my_jdbc_conn')
    
    # Get the job_id from XCom
    job_id = context['ti'].xcom_pull(task_ids='insert_task')
    
    # Dummy logic to determine success/failure
    # Replace with actual logic to check Spark job result
    spark_return_code = context['ti'].xcom_pull(task_ids='spark_submit_task')
    if spark_return_code == 0:
        status = 'SUCCESS'
        message = 'Spark job completed successfully.'
    else:
        status = 'ERROR'
        message = 'Spark job failed.'
    
    # Update query with the status and message
    update_query = """
        UPDATE job_status_table
        SET status = ?, message = ?, update_time = CURRENT_TIMESTAMP
        WHERE job_id = ?
    """
    jdbc_hook.run(update_query, parameters=(status, message, job_id))
    print(f"Updated record with job_id: {job_id} to status: {status}")

# Define default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 8, 1),
    'retries': 1
}

# Define the DAG
dag = DAG('db2_insert_update_status', default_args=default_args, schedule_interval='@daily')

# Task to insert a record into the DB2 table with a unique key
insert_task = PythonOperator(
    task_id='insert_task',
    python_callable=insert_record,
    dag=dag
)

# Task to submit a Spark job
spark_submit_task = SparkSubmitOperator(
    task_id='spark_submit_task',
    application='/path/to/your/spark_application.py',
    conn_id='spark_conn',
    dag=dag
)

# Task to update the record status based on the Spark job result
update_task = PythonOperator(
    task_id='update_task',
    python_callable=update_record_status,
    provide_context=True,
    dag=dag
)

# Define task dependencies
insert_task >> spark_submit_task >> update_task
