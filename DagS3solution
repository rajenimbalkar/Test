Ensuring resiliency in an Apache Spark and Airflow setup involves several strategies to handle failures, retries, and ensure smooth operation. Below, I'll cover resiliency features and best practices for both Spark and Airflow.

### Apache Spark Resiliency

#### 1. Data Replication
- **HDFS**: Use HDFS for storing data because it provides data replication, which ensures that data is available even if some nodes fail.
- **S3**: If using AWS S3, leverage S3’s inherent redundancy.

#### 2. Checkpointing
- **RDD Checkpointing**: Use checkpointing to truncate the lineage of RDDs, which can be very helpful in long-running applications.
- **Structured Streaming**: Use checkpointing in Structured Streaming to ensure the application can recover from failures and continue from where it left off.

```scala
streamingContext.checkpoint("hdfs:///path/to/checkpoint/dir")
```

#### 3. Task Retry
- **Spark Configurations**: Configure retries for tasks and stages.

```scala
spark.conf.set("spark.task.maxFailures", 4)
spark.conf.set("spark.stage.maxConsecutiveAttempts", 4)
```

#### 4. Dynamic Resource Allocation
- **Dynamic Allocation**: Enable dynamic allocation to add and remove executors dynamically based on the workload.

```scala
spark.conf.set("spark.dynamicAllocation.enabled", true)
```

#### 5. Fault-Tolerant Execution
- **Resilient Distributed Datasets (RDDs)**: Use RDDs which are inherently fault-tolerant.

### Apache Airflow Resiliency

#### 1. Task Retries
- **Retries**: Configure task retries with exponential backoff.

```python
default_args = {
    'owner': 'airflow',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
}
```

#### 2. Task Timeouts
- **Timeouts**: Set task timeouts to avoid hanging tasks.

```python
from airflow.utils.timeout import timeout

with DAG('my_dag', default_args=default_args) as dag:
    task = PythonOperator(
        task_id='my_task',
        python_callable=my_function,
        execution_timeout=timedelta(minutes=10)
    )
```

#### 3. Monitoring and Alerting
- **SLAs**: Define SLAs for tasks and receive alerts if tasks run longer than expected.
- **Alerting**: Use email or Slack alerts for task failures.

```python
from airflow.operators.email import EmailOperator

alert = EmailOperator(
    task_id='send_email',
    to='alert@example.com',
    subject='Airflow alert',
    html_content='Task failed: {{ task_instance.task_id }}',
    trigger_rule='one_failed'
)
```

#### 4. Scaling and HA
- **Executor**: Use the Celery Executor or Kubernetes Executor for scaling Airflow.
- **HA**: Set up multiple schedulers and web servers for high availability.

#### 5. Task Dependency Management
- **Sensors and Triggers**: Use sensors to wait for external conditions and triggers to handle task dependencies efficiently.

```python
from airflow.sensors.external_task_sensor import ExternalTaskSensor

wait_for_task = ExternalTaskSensor(
    task_id='wait_for_task',
    external_dag_id='other_dag',
    external_task_id='other_task',
    timeout=600,
    poke_interval=60
)
```

### Example DAG for Resiliency

Combining these principles, here’s an example of an Airflow DAG that processes files from an SFTP server, uploads them to S3, and processes them with Spark:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.sftp.hooks.sftp import SFTPHook
from airflow.providers.ssh.hooks.ssh import SSHHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.models import Variable
from datetime import datetime, timedelta
import os

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'execution_timeout': timedelta(minutes=60),
}

def list_files_from_sftp(sftp_conn_id, remote_path):
    sftp_hook = SFTPHook(sftp_conn_id=sftp_conn_id)
    files = sftp_hook.list_directory(remote_path)
    return [os.path.join(remote_path, file) for file in files]

def upload_to_s3(sftp_conn_id, aws_conn_id, remote_file_path, bucket_name, s3_key_prefix):
    sftp_hook = SFTPHook(sftp_conn_id=sftp_conn_id)
    s3_hook = S3Hook(aws_conn_id=aws_conn_id)

    local_file_path = os.path.join('/tmp', os.path.basename(remote_file_path))
    sftp_hook.retrieve_file(remote_file_path, local_file_path)

    s3_key = os.path.join(s3_key_prefix, os.path.basename(remote_file_path))
    s3_hook.load_file(local_file_path, key=s3_key, bucket_name=bucket_name, replace=True)
    os.remove(local_file_path)  # Clean up local file

    return s3_key

def process_file_with_spark(aws_conn_id, ssh_conn_id, s3_file_path, spark_script):
    ssh_hook = SSHHook(ssh_conn_id=ssh_conn_id)
    command = f"spark-submit {spark_script} {s3_file_path}"

    ssh_client = ssh_hook.get_conn()
    stdin, stdout, stderr = ssh_client.exec_command(command)

    exit_status = stdout.channel.recv_exit_status()
    if exit_status != 0:
        raise Exception(f"Spark job failed: {stderr.read().decode()}")

def get_task_id_from_filename(filename):
    return 'process_' + os.path.basename(filename).replace('.', '_')

with DAG(
    'sftp_to_spark_s3_resilient',
    default_args=default_args,
    description='Resilient DAG to process files from SFTP to Spark using S3',
    schedule_interval=timedelta(minutes=15),
    catchup=False,
) as dag:

    list_files_task = PythonOperator(
        task_id='list_files',
        python_callable=list_files_from_sftp,
        op_kwargs={'sftp_conn_id': 'my_sftp', 'remote_path': '/path/to/sftp/folder'},
    )

    def create_dynamic_tasks(file_list):
        for file_path in file_list:
            task_id = get_task_id_from_filename(file_path)
            upload_task = PythonOperator(
                task_id=f'upload_{task_id}',
                python_callable=upload_to_s3,
                op_kwargs={
                    'sftp_conn_id': 'my_sftp',
                    'aws_conn_id': 'aws_s3',
                    'remote_file_path': file_path,
                    'bucket_name': 'your-s3-bucket',
                    's3_key_prefix': 'spark/input'
                },
            )

            spark_task = PythonOperator(
                task_id=task_id,
                python_callable=process_file_with_spark,
                op_kwargs={
                    'aws_conn_id': 'aws_s3',
                    'ssh_conn_id': 'my_ssh',
                    's3_file_path': f's3://your-s3-bucket/spark/input/{os.path.basename(file_path)}',
                    'spark_script': '/path/to/spark_script.py',
                },
            )

            list_files_task >> upload_task >> spark_task

    dynamic_tasks = PythonOperator(
        task_id='create_dynamic_tasks',
        python_callable=create_dynamic_tasks,
        op_args=[list_files_task.output],
    )

    list_files_task >> dynamic_tasks
```

### Summary

By implementing the above strategies, you ensure your Apache Spark and Airflow workflows are resilient and capable of handling failures gracefully. The combination of retries, checkpointing, dynamic resource allocation, secure connections, monitoring, and alerting provides a robust setup for data processing and workflow management.
