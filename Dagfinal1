import re
import logging
from airflow import DAG
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.providers.jdbc.hooks.jdbc import JdbcHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.smtp.operators.smtp import EmailOperator
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup

default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
}

dag = DAG(
    's3_to_spark_with_db2_and_email',
    default_args=default_args,
    description='Read files from S3, process with Spark, update DB2, and send email notifications',
    schedule_interval=None,
)

logger = logging.getLogger(__name__)

def list_files_from_s3(bucket_name, prefix):
    s3_hook = S3Hook(aws_conn_id='my_aws_conn')
    keys = s3_hook.list_keys(bucket_name=bucket_name, prefix=prefix)
    logger.info(f'Files found in S3 for prefix {prefix}: {keys}')
    return keys

def insert_file_metadata_into_db(file_key):
    db2_hook = JdbcHook(jdbc_conn_id='db2_admin_jdbc')
    file_path, file_name = file_key.rsplit('/', 1)
    insert_query = f"""
    INSERT INTO your_table (file_path, file_name, status)
    VALUES ('{file_path}', '{file_name}', 'read')
    """
    db2_hook.run(insert_query)
    logger.info(f'Inserted metadata for file {file_name} into DB2')

def process_file_with_spark(file_key, file_type):
    task_id = re.sub(r'[^a-zA-Z0-9]', '_', file_key)
    logger.info(f'Creating Spark task for file {file_key} of type {file_type}')
    return SparkSubmitOperator(
        task_id=f'spark_submit_{file_type}_{task_id}',
        application='/path/to/your/spark/job.jar',
        java_class='com.yourcompany.YourMainClass',
        application_args=[file_type, file_key],
        conf={
            'spark.executor.extraClassPath': '/path/to/your/driver.jar',
            'spark.driver.extraClassPath': '/path/to/your/driver.jar'
        },
        dag=dag
    )

def send_email_notification(file_key, status):
    task_id = re.sub(r'[^a-zA-Z0-9]', '_', file_key)
    logger.info(f'Sending email notification for file {file_key} with status {status}')
    return EmailOperator(
        task_id=f'send_email_{task_id}',
        to='your_email@example.com',
        subject=f'File {file_key} Processing {status}',
        html_content=f'The file {file_key} has been processed with status: {status}',
        dag=dag
    )

def check_files_exist(bucket_name, prefix):
    keys = list_files_from_s3(bucket_name, prefix)
    logger.info(f'Checking for files in prefix {prefix}. Found: {keys}')
    return bool(keys)

bucket_name = 'your_bucket_name'
prefixes = ['input/filetype1', 'input/filetype2']

def create_dynamic_tasks(prefix, **kwargs):
    file_keys = list_files_from_s3(bucket_name, prefix)
    for file_key in file_keys:
        task_id = re.sub(r'[^a-zA-Z0-9]', '_', file_key)
        logger.info(f'Creating tasks for file {file_key}')

        insert_metadata_task = PythonOperator(
            task_id=f'insert_metadata_{task_id}',
            python_callable=insert_file_metadata_into_db,
            op_kwargs={'file_key': file_key},
            dag=dag,
        )

        file_type = prefix.split('/')[-1]
        spark_task = process_file_with_spark(file_key, file_type)

        email_success_task = send_email_notification(file_key, 'Success')
        email_failure_task = send_email_notification(file_key, 'Failure')

        insert_metadata_task >> spark_task >> email_success_task
        spark_task.on_failure_callback = email_failure_task.execute

for prefix in prefixes:
    with TaskGroup(group_id=f'process_files_{prefix.replace("/", "_")}', dag=dag) as group:
        check_files_task = ShortCircuitOperator(
            task_id=f'check_files_exist_{prefix.replace("/", "_")}',
            python_callable=check_files_exist,
            op_kwargs={'bucket_name': bucket_name, 'prefix': prefix},
            dag=dag,
        )

        create_dynamic_tasks_task = PythonOperator(
            task_id=f'create_dynamic_tasks_{prefix.replace("/", "_")}',
            python_callable=create_dynamic_tasks,
            op_kwargs={'prefix': prefix},
            provide_context=True,
            trigger_rule='all_success',
            dag=dag
        )

        check_files_task >> create_dynamic_tasks_task
