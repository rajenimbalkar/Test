from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.utils.dates import days_ago
import os
import logging

logger = logging.getLogger(__name__)

# Function to download the file from S3
def download_file_from_s3(bucket_name, file_key, local_dir):
    s3_hook = S3Hook(aws_conn_id='my_aws_conn')
    local_file_path = os.path.join(local_dir, file_key.replace('/', '_'))
    
    # Ensure the local directory exists
    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
    
    # Download the file from S3
    s3_hook.download_file(bucket_name=bucket_name, key=file_key, local_path=local_file_path)
    logger.info(f'Downloaded {file_key} to {local_file_path}')
    
    return local_file_path

# Function to move the file to the processed folder and delete the local file
def move_and_cleanup(bucket_name, file_key, processed_prefix, local_file_path):
    s3_hook = S3Hook(aws_conn_id='my_aws_conn')
    processed_key = os.path.join(processed_prefix, file_key)
    
    # Move the file to the processed folder
    s3_hook.copy_object(source_bucket_key=file_key, dest_bucket_key=processed_key, source_bucket_name=bucket_name, dest_bucket_name=bucket_name)
    logger.info(f'Moved {file_key} to {processed_key}')
    
    # Delete the original file from S3
    s3_hook.delete_objects(bucket_name=bucket_name, keys=[file_key])
    logger.info(f'Deleted {file_key} from bucket {bucket_name}')
    
    # Delete the local file
    if os.path.exists(local_file_path):
        os.remove(local_file_path)
        logger.info(f'Deleted local file {local_file_path}')
    else:
        logger.warning(f'Local file {local_file_path} not found.')

# Define the DAG
with DAG(
    dag_id='example_s3_processing_dag',
    default_args={
        'owner': 'airflow',
        'start_date': days_ago(1),
    },
    schedule_interval=None,
) as dag:

    bucket_name = 'your-bucket-name'
    file_key = 'input/filetype1/your-file.txt'
    processed_prefix = 'processed'
    local_dir = '/tmp/airflow_s3_files/'

    download_task = PythonOperator(
        task_id='download_file',
        python_callable=download_file_from_s3,
        op_args=[bucket_name, file_key, local_dir],
    )

    local_file_path = os.path.join(local_dir, file_key.replace('/', '_'))

    process_file_with_spark = SparkSubmitOperator(
        task_id='process_file_with_spark',
        application='path/to/your/spring-boot-application.jar',
        application_args=[local_file_path, file_key.split('/')[1]],  # Pass file path and file type as arguments
        conf={'spark.driver.extraJavaOptions': '-Dspring.profiles.active=your_profile'},
        java_class='com.example.YourMainClass',
        name='process_file_with_spark',
        packages='org.apache.spark:spark-sql_2.12:3.0.1',
        conn_id='spark_default',
    )

    cleanup_task = PythonOperator(
        task_id='move_and_cleanup',
        python_callable=move_and_cleanup,
        op_args=[bucket_name, file_key, processed_prefix, local_file_path],
    )

    download_task >> process_file_with_spark >> cleanup_task
