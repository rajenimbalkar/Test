import smtplib
from email.mime.text import MIMEText
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.utils.dates import days_ago
from airflow.models import Variable

# Define default arguments
default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
}

# Define the DAG
dag = DAG(
    'spark_submit_with_notification_dag',
    default_args=default_args,
    description='A DAG to submit Spark jobs with parameters, handle success/failure, and send notifications',
    schedule_interval=None,
)

# Email configuration
SMTP_HOST = Variable.get("smtp_host")
SMTP_PORT = Variable.get("smtp_port")
SMTP_USER = Variable.get("smtp_user")
SMTP_PASSWORD = Variable.get("smtp_password")
EMAIL_FROM = Variable.get("email_from")
EMAIL_TO = Variable.get("email_to")

def send_email(subject, body):
    msg = MIMEText(body)
    msg['Subject'] = subject
    msg['From'] = EMAIL_FROM
    msg['To'] = EMAIL_TO

    with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
        server.login(SMTP_USER, SMTP_PASSWORD)
        server.sendmail(EMAIL_FROM, [EMAIL_TO], msg.as_string())

def update_db(status, file_type):
    hook = PostgresHook(postgres_conn_id='postgres_default')
    hook.run(f"INSERT INTO job_status (file_type, status) VALUES ('{file_type}', '{status}')")

def notify_success(context, file_type):
    send_email("Spark Job Success", f"Spark job for file type {file_type} succeeded.")
    update_db("success", file_type)

def notify_failure(context, file_type):
    send_email("Spark Job Failure", f"Spark job for file type {file_type} failed.")
    update_db("failure", file_type)

# Define file types
file_types = ['filetype1', 'filetype2']

for file_type in file_types:
    submit_spark_job = SparkSubmitOperator(
        task_id=f'submit_spark_job_{file_type}',
        application='/path/to/your/spark/application.jar',
        name=f'spark_job_{file_type}',
        conf={'spark.driver.extraJavaOptions': '-Dfiletype=' + file_type},
        total_executor_cores=2,
        executor_cores=1,
        executor_memory='1g',
        driver_memory='1g',
        verbose=True,
        dag=dag,
        on_success_callback=lambda context, file_type=file_type: notify_success(context, file_type),
        on_failure_callback=lambda context, file_type=file_type: notify_failure(context, file_type),
    )

# Ensure the DAG ends with these tasks
for file_type in file_types:
    submit_spark_job
