import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.catalyst.encoders.RowEncoder;
import org.apache.spark.sql.types.*;

import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

public class SparkApp {
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("DatasetToMap")
                .master("local[*]")
                .getOrCreate();

        // Sample data
        List<String> data = Arrays.asList(
                "1,John,25",
                "2,Jane,30",
                "3,Smith,35"
        );

        // Convert list to Dataset
        Dataset<String> ds = spark.createDataset(data, Encoders.STRING());

        // Define schema and convert to Dataset<Row>
        StructType schema = new StructType(new StructField[]{
                new StructField("id", DataTypes.IntegerType, false, Metadata.empty()),
                new StructField("name", DataTypes.StringType, false, Metadata.empty()),
                new StructField("age", DataTypes.IntegerType, false, Metadata.empty())
        });

        Dataset<Row> df = ds.map((MapFunction<String, Row>) row -> {
            String[] parts = row.split(",");
            return RowFactory.create(Integer.parseInt(parts[0]), parts[1], Integer.parseInt(parts[2]));
        }, RowEncoder.apply(schema));

        // Convert rows to list of maps
        List<Map<String, Object>> result = df.collectAsList().stream().map(row -> {
            Map<String, Object> map = new HashMap<>();
            for (StructField field : schema.fields()) {
                map.put(field.name(), row.getAs(field.name()));
            }
            return map;
        }).collect(Collectors.toList());

        // Print the result
        result.forEach(System.out::println);

        spark.stop();
    }
}
