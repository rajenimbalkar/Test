import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.functions;

import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.stream.Collectors;

public class GenericFilter {

    private SparkSession spark;

    public GenericFilter(SparkSession spark) {
        this.spark = spark;
    }

    /**
     * Apply multiple `isin` filters on a Dataset<Row> based on provided column-value mappings.
     * Ignores keys that are not present as columns in the Dataset.
     *
     * @param df           The input Dataset<Row>.
     * @param filtersMap   A map of column names to lists of values to filter by.
     * @return             The filtered Dataset<Row>.
     */
    public Dataset<Row> applyFilters(Dataset<Row> df, Map<String, List<Object>> filtersMap) {
        // Get the set of columns in the dataset
        Set<String> columns = Set.of(df.columns());

        // Create a condition for each entry in the filtersMap if the column exists
        Column condition = null;

        for (Map.Entry<String, List<Object>> entry : filtersMap.entrySet()) {
            String column = entry.getKey();

            // Skip if column is not in the dataset
            if (!columns.contains(column)) {
                System.out.println("Ignoring non-existent column: " + column);
                continue;
            }

            List<Object> values = entry.getValue();
            Column currentCondition = df.col(column).isin(values.toArray());

            if (condition == null) {
                condition = currentCondition;
            } else {
                // Combine conditions with logical AND
                condition = condition.and(currentCondition);
            }
        }

        if (condition != null) {
            // Apply the combined condition
            return df.filter(functions.not(condition));
        }

        // Return original dataset if no condition was applied
        return df;
    }

    public static void main(String[] args) {
        // Initialize Spark Session
        SparkSession spark = SparkSession.builder()
                .appName("Generic Filter Example")
                .master("local[*]")
                .getOrCreate();

        // Create sample data
        List<Row> data = Arrays.asList(
                RowFactory.create("Alice", 29, "USA"),
                RowFactory.create("Bob", 35, "Canada"),
                RowFactory.create("Catherine", 28, "USA"),
                RowFactory.create("David", 40, "UK"),
                RowFactory.create("Eve", 28, "USA"),
                RowFactory.create("Frank", 35, "Canada"),
                RowFactory.create("Grace", 29, "USA"),
                RowFactory.create("Heidi", 40, "Canada")
        );

        // Define schema
        StructType schema = DataTypes.createStructType(new StructField[]{
                DataTypes.createStructField("name", DataTypes.StringType, false),
                DataTypes.createStructField("age", DataTypes.IntegerType, false),
                DataTypes.createStructField("country", DataTypes.StringType, false)
        });

        // Create a Dataset from sample data
        Dataset<Row> df = spark.createDataFrame(data, schema);

        // Show initial data
        System.out.println("Initial Data:");
        df.show();

        // Define filters
        Map<String, List<Object>> filtersMap = Map.of(
                "country", Arrays.asList("USA", "Canada"),
                "age", Arrays.asList(28, 29),
                "nonExistentColumn", Arrays.asList("value1", "value2")
        );

        // Create an instance of the GenericFilter class
        GenericFilter genericFilter = new GenericFilter(spark);

        // Apply filters
        Dataset<Row> filteredDf = genericFilter.applyFilters(df, filtersMap);

        // Show filtered data
        System.out.println("Filtered Data:");
        filteredDf.show();

        // Stop the Spark session
        spark.stop();
    }
}
