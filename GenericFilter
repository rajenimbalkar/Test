import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.functions;

import java.util.List;
import java.util.Map;
import java.util.Set;

public class DatasetFilter {

    /**
     * Apply filters to a Dataset<Row> based on a list of maps with conditions combined using AND.
     *
     * @param df            The input Dataset<Row>.
     * @param filterConditions A list of maps where each map contains column names and their respective filter values.
     * @return              The filtered Dataset<Row>.
     */
    public static Dataset<Row> applyFilters(Dataset<Row> df, List<Map<String, Object>> filterConditions) {
        // Get the set of columns in the dataset
        Set<String> columns = Set.of(df.columns());

        Column combinedCondition = null;

        // Iterate through each map of filter conditions
        for (Map<String, Object> conditions : filterConditions) {
            for (Map.Entry<String, Object> entry : conditions.entrySet()) {
                String column = entry.getKey();
                Object value = entry.getValue();

                // Skip if column is not in the dataset
                if (!columns.contains(column)) {
                    System.out.println("Ignoring non-existent column: " + column);
                    continue;
                }

                // Create a condition for the current column and value
                Column currentCondition = df.col(column).equalTo(value);

                // Combine with the existing conditions using AND
                if (combinedCondition == null) {
                    combinedCondition = currentCondition;
                } else {
                    combinedCondition = combinedCondition.and(currentCondition);
                }
            }
        }

        if (combinedCondition != null) {
            // Apply the combined condition
            return df.filter(combinedCondition);
        }

        // Return original dataset if no valid condition was applied
        return df;
    }

    public static void main(String[] args) {
        // Initialize Spark Session
        SparkSession spark = SparkSession.builder()
                .appName("Dataset Filter Example")
                .master("local[*]")
                .getOrCreate();

        // Sample data
        List<Row> data = Arrays.asList(
                RowFactory.create("Alice", 29, "USA"),
                RowFactory.create("Bob", 35, "Canada"),
                RowFactory.create("Catherine", 28, "USA"),
                RowFactory.create("David", 40, "UK"),
                RowFactory.create("Eve", 28, "USA"),
                RowFactory.create("Frank", 35, "Canada"),
                RowFactory.create("Grace", 29, "USA"),
                RowFactory.create("Heidi", 40, "Canada")
        );

        // Define schema
        StructType schema = DataTypes.createStructType(new StructField[]{
                DataTypes.createStructField("name", DataTypes.StringType, false),
                DataTypes.createStructField("age", DataTypes.IntegerType, false),
                DataTypes.createStructField("country", DataTypes.StringType, false)
        });

        // Create a Dataset from sample data
        Dataset<Row> df = spark.createDataFrame(data, schema);

        // Show initial data
        System.out.println("Initial Data:");
        df.show();

        // Define filter conditions
        List<Map<String, Object>> filterConditions = Arrays.asList(
                Map.of("country", "USA"),
                Map.of("age", 28)
        );

        // Apply filters
        Dataset<Row> filteredDf = applyFilters(df, filterConditions);

        // Show filtered data
        System.out.println("Filtered Data:");
        filteredDf.show();

        // Stop the Spark session
        spark.stop();
    }
}
