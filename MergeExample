import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Repository;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.List;
import java.util.Properties;
import java.util.stream.Collectors;

@Repository
public class EmployeeDAO {

    @Autowired
    private SparkSession sparkSession;

    private final String jdbcUrl = "jdbc:db2://<DB2_HOST>:<DB2_PORT>/<DB2_DATABASE>";
    private final Properties connectionProperties = new Properties();

    public EmployeeDAO() {
        connectionProperties.put("user", "your_db_username");
        connectionProperties.put("password", "your_db_password");
        connectionProperties.put("driver", "com.ibm.db2.jcc.DB2Driver");
    }

    /**
     * READ: Fetch all employees from the DB2 database into a Dataset<Row>.
     */
    public Dataset<Row> readEmployees() {
        return sparkSession.read()
                .jdbc(jdbcUrl, "employee_table", connectionProperties);
    }

    /**
     * WRITE: Insert new employees into the DB2 database.
     * Uses "append" mode to add new records without affecting existing ones.
     */
    public void writeEmployees(Dataset<Row> employeesDF) {
        employeesDF.write()
                .mode("append")  // Use "overwrite" to replace the table entirely
                .jdbc(jdbcUrl, "employee_table", connectionProperties);
    }

    /**
     * UPSERT: Update existing employees or insert new ones based on primary keys.
     * Uses JDBC PreparedStatement with dynamically generated MERGE SQL.
     *
     * @param employeesDF  Dataset<Row> containing employee data to upsert
     * @param primaryKeys  List of column names that form the primary key
     */
    public void upsertEmployees(Dataset<Row> employeesDF, List<String> primaryKeys) {
        if (primaryKeys == null || primaryKeys.isEmpty()) {
            throw new IllegalArgumentException("Primary keys must be provided for upsert operation.");
        }

        // Extract column names from the Dataset's schema
        List<String> columnNames = employeesDF.schema().fields()
                .stream()
                .map(field -> field.name())
                .collect(Collectors.toList());

        // Validate that primary keys exist in the schema
        for (String pk : primaryKeys) {
            if (!columnNames.contains(pk)) {
                throw new IllegalArgumentException("Primary key '" + pk + "' does not exist in the dataset schema.");
            }
        }

        // Separate primary key columns from non-primary key columns
        List<String> nonPkColumns = columnNames.stream()
                .filter(col -> !primaryKeys.contains(col))
                .collect(Collectors.toList());

        // Dynamically build the MERGE SQL statement
        StringBuilder mergeSql = new StringBuilder();
        mergeSql.append("MERGE INTO employee_table AS t ");
        mergeSql.append("USING (VALUES (");

        // Append placeholders for PreparedStatement
        String placeholders = columnNames.stream()
                .map(col -> "?")
                .collect(Collectors.joining(", "));
        mergeSql.append(placeholders).append(")) AS vals(")
                .append(String.join(", ", columnNames)).append(") ");

        // Append the ON clause based on primary keys
        mergeSql.append("ON ");
        String onClause = primaryKeys.stream()
                .map(pk -> "t." + pk + " = vals." + pk)
                .collect(Collectors.joining(" AND "));
        mergeSql.append(onClause).append(" ");

        // Append the WHEN MATCHED THEN UPDATE SET clause
        if (!nonPkColumns.isEmpty()) {
            mergeSql.append("WHEN MATCHED THEN UPDATE SET ");
            String updateSet = nonPkColumns.stream()
                    .map(col -> "t." + col + " = vals." + col)
                    .collect(Collectors.joining(", "));
            mergeSql.append(updateSet).append(" ");
        }

        // Append the WHEN NOT MATCHED THEN INSERT clause
        mergeSql.append("WHEN NOT MATCHED THEN INSERT (")
                .append(String.join(", ", columnNames))
                .append(") VALUES (")
                .append(String.join(", ", columnNames.stream()
                        .map(col -> "vals." + col)
                        .collect(Collectors.toList())))
                .append(");");

        String sql = mergeSql.toString();
        System.out.println("Generated MERGE SQL: " + sql);  // For debugging

        // Perform the upsert using foreachPartition for distributed processing
        employeesDF.foreachPartition(iterator -> {
            if (!iterator.hasNext()) return;

            try (Connection connection = DriverManager.getConnection(jdbcUrl, connectionProperties)) {
                connection.setAutoCommit(false);  // Enable transaction management

                try (PreparedStatement pstmt = connection.prepareStatement(sql)) {
                    while (iterator.hasNext()) {
                        Row row = iterator.next();

                        // Set parameters dynamically based on column order
                        for (int i = 0; i < columnNames.size(); i++) {
                            Object value = row.get(i);
                            pstmt.setObject(i + 1, value);
                        }

                        pstmt.addBatch();
                    }

                    pstmt.executeBatch();
                    connection.commit();
                } catch (SQLException e) {
                    connection.rollback();
                    e.printStackTrace();
                }
            } catch (SQLException e) {
                e.printStackTrace();
            }
        });
    }
}
