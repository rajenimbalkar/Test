The error message "String indices must be integers" suggests that somewhere in the code, a string is being indexed with another string instead of an integer. This typically happens when trying to access characters in a string using a string key, which is not allowed.

In the provided code snippet, this error might occur when accessing `file_key` from the `files` dictionary. The variable `files` is being pulled from XCom as a string rather than a dictionary. XCom stores all data as strings, so additional parsing is required to convert the string representation back into a dictionary.

### Correcting the Error

1. **Properly parse the XCom data**: Ensure that the XCom pulled data is converted back into the appropriate data structure (e.g., a dictionary).

2. **Use `ast.literal_eval` or `json.loads`**: Convert the string back into a dictionary if you expect a dictionary structure.

Here's a corrected version of the relevant part of the code:

```python
import hashlib
import ast  # Import ast to safely evaluate strings

# Function to list files and return them as a list
def list_s3_files(**context):
    s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
    files = {}
    for file_type in FILE_TYPES:
        prefix = f"{INPUT_PREFIX}/{file_type}/files/"
        file_keys = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=prefix)
        if file_keys:
            files[file_type] = file_keys
    context['ti'].xcom_push(key='files', value=files)

# DAG definition
with DAG('s3_spark_submit',
         start_date=datetime(2023, 1, 1),
         schedule_interval=None,  # Trigger manually or as required
         catchup=False) as dag:

    # List all files from input directories
    list_files_task = PythonOperator(
        task_id='list_files',
        python_callable=list_s3_files,
        provide_context=True
    )

    def create_spark_and_move_tasks(file_type, file_key):
        # Generate unique task IDs
        spark_task_id = generate_task_id(f'spark_submit_{file_type}', file_key)
        move_task_id = generate_task_id(f'move_processed_files_{file_type}', file_key)

        # Spark Submit task
        spark_submit_task = SparkSubmitOperator(
            task_id=spark_task_id,
            application='s3://path-to-your-jar/your-application.jar',
            conn_id=SPARK_CONN_ID,
            application_args=[
                '--filepath', f's3://{BUCKET_NAME}/{file_key}',
                '--filetype', file_type
            ],
            dag=dag
        )

        # Move processed files task
        move_files_task = PythonOperator(
            task_id=move_task_id,
            python_callable=move_processed_files,
            op_args=[file_key],
            provide_context=True
        )

        # Set task dependencies
        list_files_task >> spark_submit_task >> move_files_task

    # Read files list from XCom and create tasks
    files = "{{ task_instance.xcom_pull(task_ids='list_files', key='files') }}"
    files_dict = ast.literal_eval(files)  # Convert string to dict

    for file_type, file_keys in files_dict.items():
        for file_key in file_keys:
            create_spark_and_move_tasks(file_type, file_key)
```

### Key Changes:

1. **Parsing XCom Data**:
   - Use `ast.literal_eval(files)` to safely evaluate the string representation of the dictionary pulled from XCom into a dictionary object `files_dict`. This allows you to access `file_type` and `file_keys` correctly.

2. **Creating Tasks Dynamically**:
   - The `create_spark_and_move_tasks` function is defined to encapsulate the logic of creating the `SparkSubmitOperator` and `move_files_task` for each file. This ensures tasks are dynamically created with unique `task_id`s.

### Important Considerations:

- **Security**: The use of `ast.literal_eval` is preferred over `eval` for safely evaluating expressions from untrusted sources. It only evaluates literals and hence reduces security risks.

- **Error Handling**: Ensure to handle potential errors that may arise from improper parsing or missing files.

- **Testing**: Verify the logic with a subset of data to ensure that the dynamic creation of tasks works as expected and that all edge cases (e.g., no files found, parsing errors) are properly managed.
