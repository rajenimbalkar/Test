To read JSON data in a Spark application, you can use the `SparkSession` object's `read` method, which provides a convenient way to load JSON files into a DataFrame. Below is a step-by-step guide and an example:

### JSON Data Example

Let's assume you have a JSON file `people.json` with the following content:

```json
[
  {
    "name": "John Doe",
    "age": 30,
    "email": "john.doe@example.com"
  },
  {
    "name": "Jane Smith",
    "age": 25,
    "email": "jane.smith@example.com"
  }
]
```

This JSON file contains an array of JSON objects, each representing a person with `name`, `age`, and `email` fields.

### Reading JSON in a Spark Application

#### Setting Up the Spark Application

First, ensure you have the necessary Maven dependencies for Spark in your `pom.xml`:

```xml
<dependencies>
    <!-- Spark Core -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.12</artifactId>
        <version>3.0.1</version>
    </dependency>
    <!-- Spark SQL -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_2.12</artifactId>
        <version>3.0.1</version>
    </dependency>
</dependencies>
```

#### Sample Code to Read JSON

Here is a simple Spark application that reads the `people.json` file and displays its contents:

```java
package com.example.springboot;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class SparkJsonApplication implements CommandLineRunner {

    public static void main(String[] args) {
        SpringApplication.run(SparkJsonApplication.class, args);
    }

    @Override
    public void run(String... args) {
        // Create Spark Session
        SparkSession spark = SparkSession.builder()
                .appName("Spark JSON Reader")
                .master("local") // Use "yarn" for YARN mode
                .getOrCreate();

        // Read JSON file into DataFrame
        Dataset<Row> peopleDF = spark.read().json("src/main/resources/people.json");

        // Show the DataFrame content
        peopleDF.show();

        // Print the schema
        peopleDF.printSchema();

        // Stop the Spark session
        spark.stop();
    }
}
```

### Explanation

1. **SparkSession Creation**:
   - `SparkSession.builder()`: This is used to create a `SparkSession` object, which is the entry point for using DataFrame and SQL functionalities in Spark.
   - `.appName("Spark JSON Reader")`: Sets the application name.
   - `.master("local")`: Specifies that the Spark application runs locally. Use `"yarn"` for YARN mode.

2. **Reading JSON File**:
   - `spark.read().json("src/main/resources/people.json")`: Reads the JSON file located in the `src/main/resources` directory into a `DataFrame`. The path can also point to an HDFS location if running in a cluster.

3. **Displaying the Data**:
   - `peopleDF.show()`: Displays the content of the DataFrame.
   - `peopleDF.printSchema()`: Prints the schema of the DataFrame, showing the structure of the JSON data.

4. **Stopping the Spark Session**:
   - `spark.stop()`: Stops the Spark session, releasing any resources held by it.

### Handling Complex JSON

If your JSON data is more complex (nested structures), Spark's DataFrame API provides ways to handle it. You can use methods like `select`, `explode`, and nested column expressions to work with complex and nested JSON data.

### Running the Application

1. **Build the Project**: Use Maven to build the JAR file:
   ```sh
   mvn clean package
   ```

2. **Run Locally**: You can run the application locally using:
   ```sh
   java -jar target/springboot-spark-yarn-1.0.0.jar
   ```

3. **Run on a Cluster**: To run on a YARN cluster, use the `spark-submit` command:
   ```sh
   spark-submit --class com.example.springboot.SparkJsonApplication \
     --master yarn \
     target/springboot-spark-yarn-1.0.0.jar
   ```

This basic example demonstrates how to read a JSON file into a DataFrame and perform basic operations on it using Apache Spark and Spring Boot. You can extend this with more complex data transformations and actions as needed for your specific use case.
