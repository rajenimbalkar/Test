import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.util.AccumulatorV2;
import org.yaml.snakeyaml.Yaml;
import org.yaml.snakeyaml.constructor.Constructor;

import java.io.InputStream;
import java.util.*;

public class ValidationExample {
    public static void main(String[] args) {
        // Create SparkSession
        SparkSession spark = SparkSession.builder()
            .appName("ValidationExample")
            .master("local[*]")
            .getOrCreate();

        // Load validation rules
        Map<String, List<Map<String, Object>>> validationRules = loadValidationRules("/validation_rules.yaml");

        // Sample DataFrames (header, data, trailer)
        // Define your schemas and create DataFrames as shown previously

        // Initialize accumulator for error messages
        AccumulatorV2<Map<String, String>, Map<String, String>> errorAccumulator = new ErrorAccumulator();
        spark.sparkContext().register(errorAccumulator, "Error Accumulator");

        // Apply validation rules
        Dataset<Row> headerDF = applyValidationRules(spark, headerDF, validationRules.get("header"), errorAccumulator);
        Dataset<Row> dataDF = applyValidationRules(spark, dataDF, validationRules.get("data"), errorAccumulator);
        Dataset<Row> trailerDF = applyValidationRules(spark, trailerDF, validationRules.get("trailer"), errorAccumulator);

        // Show results
        headerDF.show();
        dataDF.show();
        trailerDF.show();

        // Print collected errors
        System.out.println("Validation Errors:");
        errorAccumulator.value().forEach((row, error) -> System.out.println("Row: " + row + " Error: " + error));

        // Stop SparkSession
        spark.stop();
    }

    public static Map<String, List<Map<String, Object>>> loadValidationRules(String filePath) {
        Yaml yaml = new Yaml(new Constructor(Map.class));
        try (InputStream inputStream = ValidationExample.class.getResourceAsStream(filePath)) {
            return yaml.load(inputStream);
        } catch (Exception e) {
            e.printStackTrace();
            return null;
        }
    }

    public static Dataset<Row> applyValidationRules(SparkSession spark, Dataset<Row> df, List<Map<String, Object>> rules, AccumulatorV2<Map<String, String>, Map<String, String>> errorAccumulator) {
        for (Map<String, Object> rule : rules) {
            String field = (String) rule.get("field");
            List<Map<String, Object>> fieldRules = (List<Map<String, Object>>) rule.get("rules");

            for (Map<String, Object> fieldRule : fieldRules) {
                String type = (String) fieldRule.get("type");
                String message = (String) fieldRule.get("message");
                switch (type) {
                    case "not_null":
                        df = df.filter(row -> {
                            if (row.isNullAt(row.fieldIndex(field))) {
                                errorAccumulator.add(Collections.singletonMap(row.toString(), message));
                                return false;
                            }
                            return true;
                        });
                        break;
                    case "max_length":
                        int maxLength = (int) fieldRule.get("value");
                        df = df.filter(row -> {
                            if (row.getString(row.fieldIndex(field)).length() > maxLength) {
                                errorAccumulator.add(Collections.singletonMap(row.toString(), message));
                                return false;
                            }
                            return true;
                        });
                        break;
                    case "range":
                        int min = (int) fieldRule.get("min");
                        int max = (int) fieldRule.get("max");
                        df = df.filter(row -> {
                            int value = row.getInt(row.fieldIndex(field));
                            if (value < min || value > max) {
                                errorAccumulator.add(Collections.singletonMap(row.toString(), message));
                                return false;
                            }
                            return true;
                        });
                        break;
                    case "pattern":
                        String pattern = (String) fieldRule.get("value");
                        df = df.filter(row -> {
                            if (!row.getString(row.fieldIndex(field)).matches(pattern)) {
                                errorAccumulator.add(Collections.singletonMap(row.toString(), message));
                                return false;
                            }
                            return true;
                        });
                        break;
                    // Add more cases for other types of validation rules
                }
            }
        }
        return df;
    }
}

class ErrorAccumulator extends AccumulatorV2<Map<String, String>, Map<String, String>> {
    private Map<String, String> errors = new HashMap<>();

    @Override
    public boolean isZero() {
        return errors.isEmpty();
    }

    @Override
    public AccumulatorV2<Map<String, String>, Map<String, String>> copy() {
        ErrorAccumulator newAcc = new ErrorAccumulator();
        newAcc.errors.putAll(this.errors);
        return newAcc;
    }

    @Override
    public void reset() {
        errors.clear();
    }

    @Override
    public void add(Map<String, String> v) {
        errors.putAll(v);
    }

    @Override
    public void merge(AccumulatorV2<Map<String, String>, Map<String, String>> other) {
        errors.putAll(other.value());
    }

    @Override
    public Map<String, String> value() {
        return errors;
    }
}
