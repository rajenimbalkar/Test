import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.functions;

import java.util.*;
import java.util.stream.Collectors;

public class Main {
    public static void main(String[] args) {
        // Create Spark session
        SparkSession spark = SparkSession.builder()
                .appName("FilterRowsExample")
                .master("local[*]")
                .getOrCreate();

        // Define schema for the dataset
        StructType schema = DataTypes.createStructType(new StructField[]{
                DataTypes.createStructField("id", DataTypes.StringType, false),
                DataTypes.createStructField("name", DataTypes.StringType, false),
                DataTypes.createStructField("age", DataTypes.IntegerType, false),
                DataTypes.createStructField("status", DataTypes.StringType, false)
        });

        // Example data for the dataset
        List<Row> data = Arrays.asList(
                RowFactory.create("1", "Alice", 30, "active"),
                RowFactory.create("2", "Bob", 25, "inactive"),
                RowFactory.create("3", "Charlie", 35, "active"),
                RowFactory.create("4", "David", 22, "inactive")
        );

        // Create Dataset
        Dataset<Row> dataset = spark.createDataFrame(data, schema);

        // Example list of maps to use for filtering
        List<Map<String, Object>> listOfMaps = new ArrayList<>();
        listOfMaps.add(new HashMap<String, Object>() {{
            put("name", "Alice");
            put("age", 30);
        }});
        listOfMaps.add(new HashMap<String, Object>() {{
            put("name", "Bob");
            put("status", "inactive");
        }});

        // Specify keys to filter by
        Set<String> keysToFilterBy = new HashSet<>(Arrays.asList("name", "status"));

        // Print original dataset
        System.out.println("Original Dataset:");
        dataset.show();

        // Build filter conditions for the specified keys
        String filterCondition = listOfMaps.stream()
                .map(map -> map.entrySet().stream()
                        .filter(entry -> keysToFilterBy.contains(entry.getKey()))
                        .map(entry -> {
                            String key = entry.getKey();
                            Object value = entry.getValue();
                            if (value instanceof String) {
                                return key + " = '" + value + "'";
                            } else if (value instanceof Integer) {
                                return key + " = " + value;
                            }
                            return "";
                        })
                        .collect(Collectors.joining(" AND ")))
                .collect(Collectors.joining(" OR "));

        // Apply filter condition
        Dataset<Row> filteredDataset = dataset.filter(functions.not(functions.expr(filterCondition)));

        // Print filtered dataset
        System.out.println("Filtered Dataset:");
        filteredDataset.show();

        // Stop the Spark session
        spark.stop();
    }
}
